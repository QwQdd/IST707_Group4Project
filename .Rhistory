### FILE INFO
###############################################################################
# Author: Group 4 ( Joseph Maugeri, Yu Sheng Lu , Jonathan Xi)
# Class: IST 707
# Week: 6
# Activity: Mushroom R data
# Purpose: Perform some exploratory analysis of the mushroom dataset,
# Prepare the data for several data mining techniques
#
### PACKAGES
###############################################################################
# ONCE install.packages('quanteda')
library(quanteda)
## ONCE: install.packages("tm")
library(tm) # Needed
# ONCE install.packages("textstem")
library(textstem)  ## Needed for lemmatize_strings
# ONCE install.packages("stringr")
library(stringr)
# ONCE install.packages("stringi")
library(stringi)
## ONCE: install.packages("slam")
library(slam)
##ONCE: install.packages('proxy')
library(proxy)
##ONCE: install.packages('Matrix')
library(Matrix) # Used to create document - term matrices
##ONCE: install.packages('tidytext')
library(tidytext) # convert DTM to DF
# ONCE install.packages("plyr")
library(plyr) ## for adply
# ONCE install.packages("ggplot2")
library(ggplot2)
# ONCE install.packages("wordcloud")
library(wordcloud)
# ONCE install.packages("rpart")
library(rpart)
# ONCE install.packages("rattle")
library(rattle)
# ONCE install.packages("rpart.plot")
library(rpart.plot)
# ONCE install.packages("RColorBrewer")
library(RColorBrewer)
# ONCE install.packages("Cairo")
library(Cairo)
# ONCE install.packages("network")
library(network)
##ONCE: install.packages('proxy')
library(proxy)
## ONCE:  install.packages("stringr")
library(stringr)
## ONCE: install.packages("textmineR")
library(textmineR)
## ONCE: install.packages("igraph")
library(igraph)
## ONCE: install.packages("lsa")
library(lsa)
# Libraries for Association Rule Mining
## ONCE: install.packages("arules")
library(arules)
## ONCE: install.packages("arulesViz")
library(arulesViz)
## ONCE: install.packages("datasets")
library(datasets)
### Reading in the data
###############################################################################
#setwd('C:/Users/Maugeri/Desktop/IST707')
mushrooms <- read.csv('mushrooms.csv', stringsAsFactors = TRUE)
### Exploring the Data
###############################################################################
# Using summary to get an overview
(summary(mushrooms))
# Using str() to investigate the size
(str(mushrooms))
# using sapply() with levels() to print all levels of the dataframe
M_Levels <- sapply(mushrooms,levels)
(M_Levels)
### Association Rule Mining
###############################################################################
# Lets set a seed to preserve the random numbers used in this instance
set.seed(1337)
# Adding index into the first column of DF
mushrooms <- cbind(ID = rownames(mushrooms), mushrooms)
mushrooms$ID <- factor(mushrooms$ID)
ShroomIDS <- c(mushrooms$ID)
# A copy of the data without an ID for association rule mining
mushrooms_A <- subset(mushrooms, select = -c(ID))
mushrooms_A <- subset(mushrooms_A, select = -c(veil.type))
## Testing the Generation of Association Rules
# Mine rules with the apriori algorithm.
# First look with low support, 80% confidence
mrules <- apriori(mushrooms_A, parameter = list(supp = 0.10, conf = 0.9, maxlen = 5 ))
# Summary to see what is generated initially
summary(mrules)
# Sort the rules by confidence
mrules <-sort(mrules, by="confidence", decreasing=TRUE)
# Show the top rules, but only 2 digits
options(digits=2)
arules::inspect(mrules[1:10])
top10 <- (mrules)
print(top10)
## Association Rules for class Edible Mushrooms
# Target class=e on RHS ordered by decreasing support
m_edible_rules <- apriori(mushrooms_A, parameter = list(supp = 0.10, conf = 0.9, maxlen = 5 ),
appearance = list(default="lhs",rhs="class=e"),
control = list(verbose=F))
summary(m_edible_rules)
m_edible_rules<-sort(m_edible_rules, decreasing=TRUE,by="support")
arules::inspect(m_edible_rules[1:10])
## Association Rules for class Poisonous Mushrooms
# Target class=p on RHS ordered by decreasing support
m_poisonous_rules <- apriori(mushrooms_A, parameter = list(supp = 0.10, conf = 0.9, maxlen = 5 ),
appearance = list(default="lhs",rhs="class=p"),
control = list(verbose=F))
summary(m_poisonous_rules)
m_poisonous_rules<-sort(m_poisonous_rules, decreasing=TRUE,by="support")
arules::inspect(m_poisonous_rules[1:10])
### Association Rule Mining Visualization section
###############################################################################
# visualization using aruleviz package
# Plotting the top 19 rules
top20 <- plot(mrules,method="graph",limit=20,interactive=TRUE)
top20
# plotting top 10 rules targeting rhs class=e
MEdibleViz <- plot(m_edible_rules,method="graph",limit=10,interactive=TRUE)
MEdibleViz
# plotting top 10 rules targeting rhs class=p
MPoisonousViz <- plot(m_poisonous_rules,method="graph",limit=10,interactive=TRUE)
MPoisonousViz
### Creating Training / Testing Sets
###############################################################################
### Using Sequencing to build training and testing datasets
(every4_indexes <- seq(1,nrow(mushrooms_A),4))
m_test <- mushrooms_A[every4_indexes,]
m_train <- mushrooms_A[-every4_indexes,]
m_train[1:5,6]
m_test[1:5,6]
# Storing the label of the class data and creating an unlabeld test dataframe
mtestlabels <- m_test$class
m_test_NO_LABEL <- subset(m_test, select = -c(class))
### Decision Trees model Build
###############################################################################
# Decision tree using sequenced dataframes m_test and m_train
fitM <- rpart(m_train$class ~ ., data = m_train , method="class")
summary(fitM)
fancyRpartPlot(fitM)
## Predict the Test sets
predictedM <- predict(fitM,m_test_NO_LABEL, type="class")
## Confusion Matrix
table(predictedM,mtestlabels)
'''
# Testing the model
fitT <- rpart(Ttest$Survived ~ ., data = Ttrain , method="class")
summary(fitT)
fancyRpartPlot(fitT)
## Predict the Test sets
predictedT <- predict(fitT,Ttest, type="class")
## Confusion Matrix
table(predictedT,testSurvival)
'''
## FILE INFO
################################################################################
# Author : Joseph Maugeri
# Class : IST 707
# Purpose : Week 6 Homework on Naive Bayes
### PACKAGES
################################################################################
## ONCE: install.packages("tm")
library(tm)
## ONCE: install.packages("stringr")
library(stringr)
## ONCE: install.packages("wordcloud")
library(wordcloud)
## ONCE: install.packages("slam")
library(slam)
## ONCE: install.packages("quanteda")
library(quanteda)
## ONCE: install.packages('proxy')
library(proxy)
## ONCE: install.packages('cluster')
library(cluster)
## ONCE: install.packages('tidytext')
library(tidytext) # convert DTM to DF
## ONCE: install.packages('plyr')
library(plyr) ## for adply
## ONCE: install.packages('ggplot2')
library(ggplot2)
## ONCE: install.packages('factoextra')
library(factoextra) # for fviz
## ONCE: install.packages('mclust')
library(mclust) # for Mclust EM clustering
## ONCE: install.packages('naivebayes')
library(naivebayes)
## ONCE: install.packages('e1071')
library(e1071)
## ONCE: install.packages('caret')
library(caret)
## ONCE: install.packages('rlang')
library(rlang)
## ONCE: install.packages('tm')
library(tm) # Needed
## ONCE install.packages("stringi")
library(stringi)
## ONCE: install.packages('proxy')
library(proxy)
## ONCE: install.packages('Matrix')
library(Matrix)
## ONCE install.packages("rpart")
library(rpart)
## ONCE install.packages("rattle")
library(rattle)
## ONCE install.packages("rpart.plot")
library(rpart.plot)
## ONCE install.packages("RColorBrewer")
library(RColorBrewer)
## ONCE install.packages("Cairo")
library(Cairo)
## ONCE install.packages("network")
library(network)
## ONCE: install.packages("arules")
library(arules)
## ONCE: install.packages("textmineR")
library(textmineR)
## ONCE: install.packages("igraph")
library(igraph)
## ONCE: install.packages("lsa")
library(lsa)
### DIRECTORY PATHS
################################################################################
### USE YOUR OWN PATH
# Model Building data
(K_digitsampletrain <- read.csv("Kaggle-digit-train-sample-small-1400.csv"))
(K_digitsampletest <- read.csv("Kaggle-digit-test-sample1000.csv"))
# Model testing data
(K_digittrain <- read.csv("Kaggle-digit-train.csv"))
(K_digittest <- read.csv("Kaggle-digit-test.csv"))
str(K_digitsampletrain)
str(K_digitsampletest)
summary(K_digitsampletrain)
length(K_digitsampletrain)
## Removing Unnecessary columns ( universal white space )
# removing columns in the sample training data that have no use, ( we must remove the same from he test)
K_digitsampletrain <- K_digitsampletrain %>%
dplyr::select(where(~ any(. != 0)))
# Using the names of final valid training columns in the test data
SamplePrunedCols <- colnames(K_digitsampletrain)
K_digitsampletest <- subset(K_digitsampletest, select = SamplePrunedCols)
# removing columns in the full training data that have no use, ( we will use these later)
K_digittrain <- K_digittrain %>%
dplyr::select(where(~ any(. != 0)))
# Using the names of final valid training columns in the test data
PrunedCols <- colnames(K_digittrain)
K_digittest <- subset(K_digittrain, select = PrunedCols)
## Convert the label to type factor
K_digitsampletrain$label<-as.factor(K_digitsampletrain$label)
str(K_digitsampletrain)
K_digitsampletest$label<-as.factor(K_digitsampletest$label)
str(K_digitsampletest)
K_digittrain$label<-as.factor(K_digitsampletrain$label)
str(K_digittrain)
K_digittest$label<-as.factor(K_digitsampletest$label)
str(K_digittest)
# Copying all dataframes
KDStrain <- K_digitsampletrain
KDStest <- K_digitsampletest
KDtrain <- K_digittrain
KDtest <- K_digittest
# Discretizing the dataframe for decision trees
# creating copies for use in decision tree datasets.
# Building a test function using cut() to divide age into discrete bins
cutFunc <- function(x) {
x <- cut(x, breaks = c(0,.1,15,30,45,60,75,90,105,120,135,150,165,180,195,210,225,240,Inf),labels=c("0","15","30","45","60","75","90","105","120","135","150","165","180","195","210","225","240","Inf"), include.lowest=TRUE)
return(x)
}
# this function worked on a small subset of data, but I need to write it into
# the lapply function to select all data besides the labels
KDStrain[,-1] <- lapply(KDStrain[,-1], function(x) cut(x, breaks = c(0,.1,50,100,150,200,Inf),labels=c("0","50","100","150","200","260"), include.lowest=TRUE))
KDStest[,-1] <- lapply(KDStest[,-1], function(x) cut(x, breaks = c(0,.1,50,100,150,200,Inf),labels=c("0","50","100","150","200","260"), include.lowest=TRUE))
KDtrain[,-1] <- lapply(KDtrain[,-1], function(x) cut(x, breaks = c(0,.1,50,100,150,200,Inf),labels=c("0","50","100","150","200","260"), include.lowest=TRUE))
KDtest[,-1] <- lapply(KDtest[,-1], function(x) cut(x, breaks = c(0,.1,50,100,150,200,Inf),labels=c("0","50","100","150","200","260"), include.lowest=TRUE))
### WORDCLOUDS
################################################################################
## Wordcloud require format  word  freq as columns
library(wordcloud2)
# Basic plot
(Temp2 = K_digitsampletrain)
(Temp2= Temp2[-1])
(MySums=colSums(Temp2))
(MyCols=colnames(Temp2))
(FreqDF<- data.frame(MyCols, MySums))
wordcloud2(data=FreqDF, size=.6, shape = 'star')
# Basic plot
(Temp3 = K_digitstrain)
(Temp3= Temp3[-1])
(MySums=colSums(Temp3))
(MyCols=colnames(Temp3))
(FreqDF<- data.frame(MyCols, MySums))
wordcloud2(data=FreqDF, size=.6, shape = 'star')
### NAIVE BAYES BUILDING SAMPLE MODEL w/ SUBSET OF TRAINING DATA
################################################################################
### Using Sequencing to test the small train dataset
### NAIVE BAYES USING ALL TRAINING DATA
## This will create a 1/3, 2/3 split.
X = 3
## Use these X indices to make the Testing and then
every_X_index<-seq(1,nrow(K_digitsampletrain),X)
## Sample data divided by sequencing
K_Build <- K_digitsampletrain[every_X_index, ]
K_BuildTest <-K_digitsampletrain[-every_X_index, ]
str(K_BuildTest)
## Copy the Labels
(Test_Labels <- K_BuildTest[,1])
str(Test_Labels)
## Remove the labels
(K_BuildTest_NO_LABEL <- K_BuildTest[,-c(1)])
(K_BuildTest_NO_LABEL[5,])
### RUN Naive Bayes on small data build test
#K_Build
NB_e1071_2<-naiveBayes(K_Build, K_Build$label, laplace = 1)
NB_e1071_Pred <- predict(NB_e1071_2, K_BuildTest_NO_LABEL)
#NB_e1071_2
table(NB_e1071_Pred,Test_Labels)
(NB_e1071_Pred)
##Visualize the model building
plot(NB_e1071_Pred)
# Creating a dataframe of a matrix of the confusion matrix
(conf.mat <- caret::confusionMatrix(NB_e1071_Pred, Test_Labels))
confdf <- as.data.frame(as.matrix(conf.mat))
# Viewing the column sums ( predict class)
(confcolSums <- colSums(confdf))
# Viewing the row sums ( actual class )
(confrowSums <- rowSums(confdf))
### NAIVE BAYES ALL SAMPLE TRAINING DATA
################################################################################
## Using the sample training data with sample test data
# some formatting
K_Sampletrain <- K_digitsampletrain
K_Sampletest <- K_digitsampletest
## Copy the Labels
(S_Test_Labels <- K_Sampletest[,1])
str(S_Test_Labels)
## Remove the labels
(K_S_Test_NO_LABEL <- K_Sampletest[,-c(1)])
(K_S_Test_NO_LABEL[5,])
### RUN Naive Bayes on sample data
NB_e1071_25 <- naiveBayes(K_Sampletrain, K_Sampletrain$label, laplace = 1)
NB_e1071_25Pred <- predict(NB_e1071_25, K_S_Test_NO_LABEL)
#NB_e1071_25
table(NB_e1071_25Pred,S_Test_Labels)
(NB_e1071_25Pred)
##Visualize the model building
plot(NB_e1071_25Pred)
### NAIVE BAYES ALL TRAINING DATA
################################################################################
## Running the model on the larger dataset ( same code is used, but now we are
# going to predict the sampletest dataset using the entire sampletrain dataset)
K_train <- K_digittrain
K_test <-K_digittest
## Make sure label is factor type
str(K_test)
## Copy the Labels
(K_test_Labels <- K_test[,1])
str(K_test_Labels)
## Remove the labels
(K_test_NO_LABEL <- K_test[,-c(1)])
K_test_NO_LABEL
### RUN Naive Bayes on the training data
K_train
NB_e1071_3<-naiveBayes(K_train, K_train$label, laplace = 1)
NB_e1071_3_Pred <- predict(NB_e1071_3, K_test_NO_LABEL)
#NB_e1071_3
table(NB_e1071_3_Pred,K_test_Labels)
(NB_e1071_3_Pred)
##Visualize the model
plot(NB_e1071_3_Pred)
## using naivebayes package
## https://cran.r-project.org/web/packages/naivebayes/naivebayes.pdf
NB_object<- naive_bayes(K_train$label ~ ., data=K_train, laplace = 1)
NB_object
NB_prediction<-predict(NB_object, K_test_NO_LABEL)
#NB_prediction
#head(predict(NB_object, DF_Test_NO_LABEL))
table(NB_prediction,K_test_Labels)
plot(NB_prediction)
## Confusion Matrix
### USING DECISION TREE ON TRAING SAMPLE DATA ONLY
################################################################################
X = 3
## Use these X indices to make the Testing and then
Devery_X_index<-seq(1,nrow(KDStrain),X)
## Sample data divided by sequencing
KDStrainbuild <- KDStrain[every_X_index, ]
KDStraintest <-KDStrain[-every_X_index, ]
str(KDStraintest)
## Copy the Labels
(KDStraintestlabels <- KDStraintest[,1])
str(KDStraintestlabels)
## Remove the labels
(KDStraintest_NO_LABEL <- KDStraintest[,-c(1)])
(KDStraintest_NO_LABEL[5,])
## Decision Trees using training sample data
fitKBS <- rpart(KDStrainbuild$label ~ ., data = KDStrainbuild, method="class")
summary(fitKBS)
## Predict the Test sets
predictedKBS <- predict(fitKBS,KDStraintest_NO_LABEL, type="class")
## Confusion Matrix
table(predictedKBS,KDStraintestlabels)
# visualize the results
fancyRpartPlot(fitKBS)
# Creating a dataframe of a matrix of the confusion matrix
(conf.mat.D <- caret::confusionMatrix(predictedKBS, KDStraintestlabels))
confdfD <- as.data.frame(as.matrix(conf.mat.D))
# Viewing the column sums ( predict class)
(confDcolSums <- colSums(confdfD))
# Viewing the row sums ( actual class )
(confDrowSums <- rowSums(confdfD))
### USING DECISION TREE ON TRAINING AND TESTING SAMPLE DATA
################################################################################
## Copy the test Labels (of factor datasets made prior)
(kDStestfactorLABELs <- KDStest[,1])
str(kDStestfactorLABELs)
## Remove the labels
(kDS_test_factor_No_LABEL <- KDStest[,-c(1)])
kDS_test_factor_No_LABEL
## Decision Trees using training sample data to predict test data
fitKS <- rpart(KDStrain$label ~ ., data = KDStrain, method="class")
summary(fitKS)
## Predict the Test sets
predictedKS <- predict(fitKS,kDS_test_factor_No_LABEL, type="class")
## Confusion Matrix
table(predictedKS,kDStestfactorLABELs)
# visualize the results
fancyRpartPlot(fitKS)
## Predict the Test sets
predictedKS <- predict(fitKS,kDS_test_factor_No_LABEL, type="class")
## Confusion Matrix
table(predictedKS,kDStestfactorLABELs)
# visualize the results
fancyRpartPlot(fitKS)
### USING DECISION TREE ON ALL TRAINING AND TEST DATA
################################################################################
## Decision Trees using all data
fitKB <- rpart(KDtrain$label ~ ., data = KDtrain, method="class")
summary(fitKB)
## Copy the test Labels (of factor datasets made prior)
(kDtestfactorLABELs <- KDtest[,1])
str(K_test_Labels)
## Remove the labels
(kD_test_factor_No_LABEL <- KDtest[,-c(1)])
kD_test_factor_No_LABEL
## Predict the Test sets
predictedKB <- predict(fitKB,kD_test_factor_No_LABEL, type="class")
## Confusion Matrix
table(predictedKB,kDtestfactorLABELs)
# visualize the results
fancyRpartPlot(fitKB)
